---
title: "Práctica 1 Aprendizaje Automático"
author: "Antonio Álvarez Caballero"
output: pdf_document
---
```{r, include = FALSE}
library(knitr)
opts_chunk$set(tidy.opts=list(width.cutoff=70))
set.seed(123456)
```
# Generación y visualización de datos

## Ejercicio 1

En primer lugar, creamos una función que crea un data.frame con valores aleatorios según una distribución uniforme.
_N_ es el número de filas del data.frame, _dim_ el número de columnas y _rango_ el rango donde estarán los valores.

``` {r uniforme}
simula_unif <- function(N, dim, rango){
  res <- data.frame(matrix(nrow = N, ncol = dim))

  for(i in 1:N){
    res[i,] <- runif(dim, rango[1], rango[length(rango)])
  }
  
  names(res) <- c("X", "Y")

  return(res)
}
```

## Ejercicio 2

Del mismo modo creamos una función análoga para la distribución _normal_ o _gaussiana_.


``` {r gauss}
simula_gauss <- function(N, dim, sigma){
  res <- data.frame(matrix(nrow = N, ncol = dim))

  for(i in 1:N){
    res[i,] <- rnorm(dim, sd = sqrt(sigma))
  }
  
  names(res) <- c("X", "Y")

  return(res)
}
```

## Ejercicios 3 y 4

Ahora asignamos los resultados a sendos objetos del tipo _data.frame_ y las dibujamos.

``` {r datos}
muestra.uniforme <- simula_unif(50, 2, -50:50)
muestra.gaussiana <- simula_gauss(50, 2, 5:7)
```

\newpage

``` {r plotUniforme}
plot(muestra.uniforme, main = "Distribución uniforme", xlab = "", ylab = "")
```


```{r plotNormal}
plot(muestra.gaussiana, main = "Distribución normal", xlab = "", ylab = "")
```

\newpage
## Ejercicio 5

Ahora escribimos una pequeña función para calular una recta dados dos puntos. Daremos la recta con la ecuación punto pendiente, por lo que tenemos que calcular la pendiente y la desviación.

```{r calculaRecta}
simulaRecta <- function(intervalo){
  A <- runif(2, intervalo[1], intervalo[length(intervalo)])
  B <- runif(2, intervalo[1], intervalo[length(intervalo)])
  
  m <- (A[2] - B[2]) / (A[1] - B[1])
  b <- A[2] - m * A[1]
  
  return(c(b,m))
}


```

Generamos una recta aleatoria en $[-50,50]$

```{r simulaRecta}
rectaPrueba <- simulaRecta(-50:50)

plot(1, type="n", xlab="", ylab="", xlim=c(-50, 50), ylim=c(-50, 50))
abline(coef = rectaPrueba)
```

\newpage

## Ejercicio 6

Ahora clasificamos los datos de la muestra uniforme según otra recta aleatoria. Guardamos las muestras clasificadas para próximos ejercicios.

```{r plotClasificadoRecta, tidy = TRUE}
rectaClasificacion <- simulaRecta(-50:50)

muestra.uniforme.etiquetada <- cbind(muestra.uniforme, Etiqueta = sign(muestra.uniforme$Y - rectaClasificacion[2]*muestra.uniforme$X - rectaClasificacion[1]))

muestra.uniforme.positiva <- subset(muestra.uniforme.etiquetada, Etiqueta ==  1)

muestra.uniforme.negativa <- subset(muestra.uniforme.etiquetada, Etiqueta == -1)

formas <- ifelse(muestra.uniforme.etiquetada$Etiqueta == 1, "+" , "-")
colores <- ifelse(muestra.uniforme.etiquetada$Etiqueta == 1, "red" , "blue")

plot(muestra.uniforme, 
     pch = formas,
     col = colores)

abline(coef = rectaClasificacion)
```

\newpage
## Ejercicio 7

Una vez hemos clasificado los datos con una recta, clasificamos con otro tipo de funciones.

```{r plotClasificadoF1, echo = FALSE}

f <- function(x,y) {(x-10)^2 + (y-20)^2 - 400}

x <- seq(-50,50,length=1000)
y <- seq(-50,50,length=1000)
z <- outer(x,y,f)

contour(x,y,z,levels=0,drawlabels=FALSE, main=expression((x-10)^2 + (y-20)^2 - 400), xlab="", ylab="", xlim=c(-50, 50), ylim=c(-50, 50))

points(subset(muestra.uniforme, f(X,Y) > 0 ), 
       pch = "+", col = "red")
points(subset(muestra.uniforme, f(X,Y) <= 0 ), 
       pch = "-", col = "blue")

```

```{r plotClasificadoF2, echo = FALSE}

f <- function(x,y) {0.5*(x+10)^2 + (y-20)^2 - 400}

x <- seq(-50,50,length=1000)
y <- seq(-50,50,length=1000)
z <- outer(x,y,f)

contour(x,y,z,levels=0,drawlabels=FALSE, main=expression(0.5*(x+10)^2 + (y-20)^2 - 400), xlab="", ylab="", xlim=c(-50, 50), ylim=c(-50, 50))

points(subset(muestra.uniforme, f(X,Y) > 0 ), 
       pch = "+", col = "red")
points(subset(muestra.uniforme, f(X,Y) <= 0 ), 
       pch = "-", col = "blue")

```

```{r plotClasificadoF3, echo = FALSE}

f <- function(x,y) {0.5*(x-10)^2 - (y+20)^2 - 400}

x <- seq(-50,50,length=1000)
y <- seq(-50,50,length=1000)
z <- outer(x,y,f)

contour(x,y,z,levels=0,drawlabels=FALSE, main=expression(0.5*(x-10)^2 - (y+20)^2 - 400), xlab="", ylab="", xlim=c(-50, 50), ylim=c(-50, 50))

points(subset(muestra.uniforme, f(X,Y) > 0 ), 
       pch = "+", col = "red")
points(subset(muestra.uniforme, f(X,Y) <= 0 ), 
       pch = "-", col = "blue")

```

```{r plotClasificadoF4, echo = FALSE}

f <- function(x,y) {y - 20*x^2 -5*x + 3}

x <- seq(-50,50,length=1000)
y <- seq(-50,50,length=1000)
z <- outer(x,y,f)

contour(x,y,z,levels=0,drawlabels=FALSE, main=expression(y - 20*x^2 -5*x + 3), xlab="", ylab="", xlim=c(-50, 50), ylim=c(-50, 50))

points(subset(muestra.uniforme, f(X,Y) > 0 ), 
       pch = "+", col = "red")
points(subset(muestra.uniforme, f(X,Y) <= 0 ), 
       pch = "-", col = "blue")

```

Añadir aquí conclusiones de las regiones nuevas comparadas con la lineal.

\newpage
## Ejercicio 8

Tomamos la muestra clasificada y creamos un vector de booleanos con el 10% _TRUE_ y el restante _FALSE_. Así luego extraeremos una porción de la muestra.

```{r cambiaMuestra, tidy = TRUE}

# Tomamos la porción deseada
porcion <- 10
numero.datos.positivos <- ceiling(nrow(muestra.uniforme.positiva) * porcion / 100)
numero.datos.negativos <- ceiling(nrow(muestra.uniforme.negativa) * porcion / 100)


vector.aleatorio.muestras.positivas <- c(rep(FALSE,numero.datos.positivos), 
                                         rep(TRUE, nrow(muestra.uniforme.positiva) - numero.datos.positivos))

vector.aleatorio.muestras.negativas <- c(rep(FALSE,numero.datos.negativos), 
                                         rep(TRUE, nrow(muestra.uniforme.negativa) - numero.datos.negativos))

vector.aleatorio.muestras.positivas <- sample(vector.aleatorio.muestras.positivas)
vector.aleatorio.muestras.negativas <- sample(vector.aleatorio.muestras.negativas)

muestra.uniforme.positiva.nueva <- 
  muestra.uniforme.positiva[vector.aleatorio.muestras.positivas,]
muestra.uniforme.negativa.nueva <- 
  muestra.uniforme.negativa[vector.aleatorio.muestras.negativas,]

muestra.uniforme.positiva.nueva <- rbind(muestra.uniforme.positiva.nueva,
                                       muestra.uniforme.negativa[!vector.aleatorio.muestras.negativas,])

muestra.uniforme.negativa.nueva <- rbind(muestra.uniforme.negativa.nueva,
                                         muestra.uniforme.positiva[!vector.aleatorio.muestras.positivas,])

muestra.uniforme.positiva.nueva$Etiqueta <-  1
muestra.uniforme.negativa.nueva$Etiqueta <- -1

```

Ahora mostramos los nuevos resultados

```{r nuevaMuestraClasificada, echo = FALSE}

plot(1, type="n", xlab="", ylab="", xlim=c(-50, 50), ylim=c(-50, 50))

points(muestra.uniforme.positiva.nueva, pch = "+", col = "red")
points(muestra.uniforme.negativa.nueva, pch = "-", col = "blue")

abline(coef = rectaClasificacion)

```

# Ajuste del algoritmo Perceptron (PLA)

## Ejercicio 1

Vamos a escribir una función que implemente el algoritmo _Perceptron_ o _PLA_. _Datos_ es un _data.frame_ con cada muestra por fila, _label_ el vector de etiquetas, el cual debe tener la misma longitud que los datos de muestra, *max_iter* es el número máximo de iteraciones del algoritmo y _vini_ es el vector inicial, que debe tener la misma dimensión que los datos.


```{r funcionPLA}
ajusta_PLA <- function(datos, label, max_iter, vini){
  cambio <- TRUE
  w <- vini
  iteraciones <- 0
  errores <- 0
  X <- cbind(1,datos)
  
  while (cambio && iteraciones < max_iter){
    cambio <- FALSE
    errores <- 0
    for (i in 1:nrow(datos)){
      x_i <- as.numeric(X[i,])
      prodEscalar <- crossprod(w,x_i)
      if (sign(prodEscalar) != label[i]){
        cambio <- TRUE
        errores <- errores + 1
        w <- w + label[i] * x_i
      }
    }
    iteraciones <- iteraciones + 1
  }

  resultado <- list("Peso inicial" = vini ,"Pesos" = w, 
                    "Iteraciones" = iteraciones, "Errores" = errores,
                    "Recta" = c(-w[1]/w[3], -w[2]/w[3]) )
  return (resultado)
}
```

\newpage
## Ejercicio 2

Vamos a hacer una simulación de prueba. Tomamos los datos de la muestra uniforme y un vector de etiquetas aleatorio.

```{r clasificacionPruebaPLA}

etiquetas <- t(as.vector(muestra.uniforme.etiquetada["Etiqueta"]))
vini <- rep(0,3)

resultado <- ajusta_PLA(muestra.uniforme, etiquetas, 100, vini)

w <- as.vector(resultado$Pesos)
iteraciones <- as.numeric(resultado$Iteraciones)

print(iteraciones)

mediaIteraciones <- 0

for (i in 1:10){
  vini <- runif(3, 0,1)
  print(vini)
  resultado <- ajusta_PLA(muestra.uniforme, etiquetas, 100, vini) 
  mediaIteraciones <- mediaIteraciones + as.numeric(resultado$Iteraciones)
  print(resultado$Iteraciones)
}
mediaIteraciones <- mediaIteraciones / 10

print(mediaIteraciones)
```


De estos resultados podemos decir que si los datos son separables, el _PLA_ converge rápidamente.

## Ejercicio 3

Vamos a ejecutar el _PLA_ con un conjunto de datos no separable.

```{r clasificacionNoSeparable}
muestra.uniforme.no.separable <- rbind(cbind(muestra.uniforme.positiva.nueva),
                                       cbind(muestra.uniforme.negativa.nueva))

rownames(muestra.uniforme.no.separable) <- NULL

etiquetas <- t(as.vector(muestra.uniforme.no.separable["Etiqueta"]))

vini <- rep(0,3)

resultado <- ajusta_PLA(muestra.uniforme.no.separable[,c("X","Y")], etiquetas, 10, vini)
print(resultado$Errores)

resultado <- ajusta_PLA(muestra.uniforme.no.separable[,c("X","Y")], etiquetas, 100, vini)
print(resultado$Errores)

resultado <- ajusta_PLA(muestra.uniforme.no.separable[,c("X","Y")], etiquetas, 1000, vini)
print(resultado$Errores)
```

Visto los resultados, podemos afirmar que sobre un conjunto no separable de datos, el _PLA_ no converge y por tanto no da buenas soluciones por muchas iteraciones que le permitamos.

\newpage
## Ejercicio 4

Tomemos la función cuadrática del Ejercicio 7 y realicemos el mismo procedimiento.

```{r clasificacionCircular}
f <- function(x,y) {(x-10)^2 + (y-20)^2 - 400}

muestra.uniforme.circular <- cbind(muestra.uniforme, 
                                   Etiqueta = sign(f(muestra.uniforme$X, muestra.uniforme$Y)))

etiquetas <- t(as.vector(muestra.uniforme.circular["Etiqueta"]))

vini <- rep(0,3)

resultado <- ajusta_PLA(muestra.uniforme.circular[,c("X","Y")], etiquetas, 10, vini)
print(resultado$Errores)

resultado <- ajusta_PLA(muestra.uniforme.circular[,c("X","Y")], etiquetas, 100, vini)
print(resultado$Errores)

resultado <- ajusta_PLA(muestra.uniforme.circular[,c("X","Y")], etiquetas, 1000, vini)
print(resultado$Errores)

plot(1, type="n", xlab="", ylab="", xlim=c(-50, 50), ylim=c(-50, 50))

points(subset(muestra.uniforme.circular,Etiqueta ==  1), pch = "+", col = "red")
points(subset(muestra.uniforme.circular,Etiqueta == -1), pch = "-", col = "blue")

abline(coef = resultado$Recta)
```

Con este conjunto de datos, en los que la separabilidad tiene forma circular, el _PLA_ no sirve absolutamente de nada: contra más iteraciones, menos clasifica al dejar todos los datos a un lado.

\newpage
## Ejercicio 5

Vamos a modificar la función *ajusta_PLA* para que vaya haciendo plot cada iteración.

```{r ajusta_PLA_Plot}

ajusta_PLA_PLOT <- function(datos, label, max_iter, vini){
  cambio <- TRUE
  w <- vini
  iteraciones <- 0
  errores <- 0
  X <- cbind(1,datos)
  
  datos.etiquetados <- cbind(datos, Etiqueta = t(label))

  while (cambio && iteraciones < max_iter){
    cambio <- FALSE
    errores <- 0
    for (i in 1:nrow(datos)){
      x_i <- as.numeric(X[i,])
      prodEscalar <- crossprod(w,x_i)
      if (sign(prodEscalar) != label[i]){
        cambio <- TRUE
        errores <- errores + 1
        w <- w + label[i] * x_i
      }
    }
    ### PLOT
    recta <- c(-w[1]/w[3], -w[2]/w[3])
    plot(0, type="n", xlab="", ylab="", xlim=c(-50, 50), ylim=c(-50, 50))

    points(subset(datos.etiquetados,Etiqueta ==  1), pch = "+", col = "red")
    points(subset(datos.etiquetados,Etiqueta == -1), pch = "-", col = "blue")
    
    abline(coef = recta)
    
    ##############
    iteraciones <- iteraciones + 1
    print(paste("Iteracion", iteraciones))
    print(paste("Errores:",errores))
  }

  resultado <- list("Peso inicial" = vini ,"Pesos" = w, 
                    "Iteraciones" = iteraciones, "Errores" = errores,
                    "Recta" = c(-w[1]/w[3], w[2]/w[3]) )
  return (resultado)
}

vini <- rep(0,3)

resultado <- ajusta_PLA_PLOT(muestra.uniforme.circular[,c("X","Y")], etiquetas, 15, vini)
print(resultado$Errores)
```

\newpage

A la vista de estos resultados, proponemos la siguiente mejora: guardar el mejor resultado en una variable y devolverlo.



```{r funcionPLAMOD}
ajusta_PLA_MOD <- function(datos, label, max_iter, vini){
  cambio <- TRUE
  w <- vini
  iteraciones <- 0
  errores <- 0
  X <- cbind(1,datos)
  mejor <- vini
  mejor_errores <- nrow(datos)
  
  while (cambio && iteraciones < max_iter){
    cambio <- FALSE
    errores <- 0
    for (i in 1:nrow(datos)){
      x_i <- as.numeric(X[i,])
      prodEscalar <- crossprod(w,x_i)
      if (sign(prodEscalar) != label[i]){
        cambio <- TRUE
        errores <- errores + 1
        w <- w + label[i] * x_i
      }
    }
    if (errores < mejor_errores){
      mejor <- w
      mejor_errores <- errores
    }
    iteraciones <- iteraciones + 1
  }

  resultado <- list("Peso inicial" = vini ,"Pesos" = mejor, 
                    "Iteraciones" = iteraciones, "Errores" = mejor_errores,
                    "Recta" = c(-w[1]/w[3], -w[2]/w[3]) )
  return (resultado)
}
```

Si lo ejecutamos con el mismo conjunto de antes nos devuelve

```{r pruebaMOD}
resultado <- ajusta_PLA_MOD(muestra.uniforme.circular[,c("X","Y")], etiquetas, 15, vini)
print(resultado$Errores)
```

\newpage

# Regresión lineal

```{r leerZip}
archivo  <- read.csv("zip.train", header = FALSE, sep = " ")
colnames(archivo)[1] <- "ID"
```

