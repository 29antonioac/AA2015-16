---
title: "Trabajo 3"
author: "Antonio Álvarez Caballero"
date: "27 de mayo de 2016"
output: pdf_document
---

```{r setup, include=FALSE}
require(ISLR)
require(ggplot2)
require(plotly)
require(reshape)
require(GGally)
require(ROCR)
require(MESS)
require(boot)
require(MASS)
require(glmnet)
require(e1071)
require(caret)
require(randomForest)
require(gbm)
require(tree)
require(knitr)

set.seed(123456789)
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)

mse <- function(sim, obs) mean( (sim - obs)^2, na.rm = TRUE)
```

# Ejercicio 1

```{r Auto, echo = FALSE, cache = FALSE}
attach(Auto)
```
```{r Auto2, echo = FALSE}
pairs(Auto)
```

## Apartado a)

Parece ser que las variables de las que más depende _mpg_ son _displacement_, _horsepower_ y _weight_. Veámoslas con más detalle.

```{r plots, echo = FALSE, cache = TRUE}
ggpairs(Auto, columns =  c("mpg","displacement", "horsepower", "weight"))
# ggplot(Auto, aes(x=cylinders,y=mpg)) + geom_boxplot(aes(group = cut_width(cylinders,0.25)))
ggplot(Auto, aes(x=displacement,y=mpg)) + geom_boxplot(aes(group = cut_width(displacement,0.25)))
ggplot(Auto, aes(x=horsepower,y=mpg)) + geom_boxplot(aes(group = cut_width(horsepower,0.25)))
ggplot(Auto, aes(x=weight,y=mpg)) + geom_boxplot(aes(group = cut_width(weight,0.25)))
# ggplot(Auto, aes(x=acceleration,y=mpg)) + geom_boxplot(aes(group = cut_width(acceleration,0.25)))
# ggplot(Auto, aes(x=year,y=mpg)) + geom_boxplot(aes(group = cut_width(year,0.25)))
# ggplot(Auto, aes(x=origin,y=mpg)) + geom_boxplot(aes(group = cut_width(origin,0.25)))
# ggplot(Auto, aes(x=name,y=mpg)) + geom_boxplot(aes(group = cut_width(name,0.25)))
```

Es claro que existe una dependencia entre estas variables. Además, con el primer plot podemos ver las correlaciones entre estas 3 variables, que es alta.

## Apartado b)

Seleccionamos las variables que hemos decidido para predecir.

```{r selection}
Auto.selected <- Auto[,c("displacement","horsepower","weight")]
```

## Apartado c)

Como nuestro conjunto de datos es grande (```r I(nrow(Auto))``` instancias), podemos realizar un muestreo aleatorio. Así tampoco falseamos las muestras, cosa que podría pasarnos si realizamos un muestreo estratificado.

```{r split}
index <- sample(nrow(Auto), size = 0.8*nrow(Auto) )

Auto.train <- Auto.selected[index,]
Auto.test <- Auto.selected[-index,]
```

## Apartado d)

Vamos a crear una nueva variable, _mpg01_, la cual tendrá 1 si el valor de _mpg_ está por encima de la mediana y -1 en otro caso.

```{r mpg01}
mpg01 <- ifelse(Auto$mpg >= median(Auto$mpg), 1, 0)
Auto.selected$mpg01 <- mpg01
Auto.train$mpg01 <- mpg01[index]
Auto.test$mpg01 <- mpg01[-index]
```

### Apartado d1)

Vamos a ajustar un modelo de regresión logística para predecir _mpg01_.

```{r regLog}
model.LogReg <- glm(mpg01 ~ ., data = Auto.train, family = binomial)
prediction.LogReg <- predict(model.LogReg, newdata = Auto.test, type = "response")
prediction.LogReg.labels <- ifelse(prediction.LogReg > 0.5, 1, 0)
error.test <- sum(sign(prediction.LogReg.labels) != sign(Auto.test$mpg01)) / 
                                                  length(prediction.LogReg.labels)
```

El error de test de este modelo es ```r I(error.test*100)```.

### Apartado d2)

Ahora vamos a ajusart un modelo k-NN.

```{r kNN}

```

### Apartado d3)

Veamos las curvas ROC de ambos modelos.


```{r ROC}
roc.prediction.regLog <- prediction(prediction.LogReg, Auto.test$mpg01)
roc.performance.regLog <- performance(roc.prediction.regLog, measure = "tpr", x.measure = "fpr")

### Meter del knn

####
roc.performance.knn <- roc.performance.regLog

roc.data <- data.frame(x  = roc.performance.regLog@x.values[[1]],
                       y1 = roc.performance.regLog@y.values[[1]],
                       y2 = roc.performance.knn@y.values[[1]])

ggplot(roc.data, aes(x)) + 
  geom_line(aes(y = y1, colour = "Logistic Regression")) +
  geom_line(aes(y = y2, colour = "k-NN")) +
  theme(legend.title = element_blank()) + 
  labs(title= "ROC curves", x = "False positive rate", y = "True positive rate")

auc.regLog  <- auc(roc.data$x,roc.data$y1, type = 'spline')
auc.knn     <- auc(roc.data$x,roc.data$y2, type = 'spline')
```

El área bajo la curva de la ROC de regresión logística es ```r I(auc.regLog)``` y la del k-NN es ```r I(auc.knn)```. Luego ```r if(auc.regLog > auc.knn) "regresión logística" else "k-NN"``` es el modelo que mejor _performance_ tiene.

## Apartado e) (Bonus-1)

Para estudiar el error con validación cruzada hacemos uso de `cv.glm`

```{r cv.glm}
model.full.LogReg <- glm(mpg01 ~ ., data = Auto.selected)
cv.LogReg <- cv.glm(data = Auto.selected, glmfit = model.full.LogReg, K = 5)
cv.LogReg$delta
```

El error estimado es el primero de este vector. El segundo es un ajuste para compensar el sesgo introducido al no usar *Leave-One-Out*.

Para el caso del k-NN

```{r cv.knn}

```

Por tanto, vemos que es mejor _uno_.

## Apartado f) (Bonus-2)

Por hacer

# Ejercicio 2

## Apartado a)

Ajustamos con validación cruzada sobre la variable _crim_, que es la que está en la posición 1.

```{r cache = FALSE}
attach(Boston)

```


```{r boston}
set.seed(123456789)
index <- sample(nrow(Boston), 0.8*nrow(Boston))
Boston.full <- Boston
Boston.train <- Boston[index,]
Boston.test <- Boston[-index,]

model.Boston <- glmnet(as.matrix(Boston.train[,-1]),Boston.train[,1], alpha = 1)

```


## Apartado b)

Ahora utilizamos un método LASSO y seleccionamos las variables que están por encima de un umbral.

```{r LASSO}
cv.Boston <- cv.glmnet(as.matrix(Boston[,-1]), Boston[,1], nfolds = 5, alpha = 1)
lasso.coeff <- predict(cv.Boston, type="coefficients", s = cv.Boston$lambda.min)
threshold <- 0.1
selected <- which(abs(lasso.coeff) > threshold)[-1]
```

Con esto afirmamos que las características que superan nuestro umbral ```r I(threshold)``` son ```r I(selected)```.

Seguir con regularización.

## Apartado c)

Al igual que en el anterior apartado, definimos una nueva variable usando la mediana como umbral.

```{r crim1, cache = TRUE}
Boston.full$crim <- ifelse(Boston$crim > median(Boston$crim), 1, -1)
Boston.train.crim1 <- Boston.full[index,]
Boston.test.crim1 <- Boston.full[-index,]
pairs(Boston.full)
```

Ahora ajustamos varias _SVM_, probaremos la lineal y con los núcleos disponibles, y veremos cómo se comporta cada uno.
Posiblemente el núcleo lineal no sea suficiente, ya que los datos no parecen lo suficientemente separados. A priori parece que uno polinomial hará un mejor trabajo.

```{r SVM}
svm.linear <- svm(crim ~ ., data = Boston.train.crim1, kernel = "linear")
svm.linear.prediction <- predict(svm.linear, newdata = Boston.test.crim1[,-1])
confusionMatrix(sign(svm.linear.prediction), Boston.test.crim1$crim)

svm.polynomial <- svm(crim ~ ., data = Boston.train.crim1, kernel = "polynomial")
svm.polynomial.prediction <- predict(svm.polynomial, newdata = Boston.test.crim1[,-1])
confusionMatrix(sign(svm.polynomial.prediction), Boston.test.crim1$crim)

svm.radial <- svm(crim ~ ., data = Boston.train.crim1, kernel = "radial")
svm.radial.prediction <- predict(svm.radial, newdata = Boston.test.crim1[,-1])
confusionMatrix(sign(svm.radial.prediction), Boston.test.crim1$crim)

svm.sigmoid <- svm(crim ~ ., data = Boston.train.crim1, kernel = "sigmoid")
svm.sigmoid.prediction <- predict(svm.sigmoid, newdata = Boston.test.crim1[,-1])
confusionMatrix(sign(svm.sigmoid.prediction), Boston.test.crim1$crim)

```

Al final, el kernel polinomial ha sido el que mejor ha funcionado. El kernel lineal se comporta muy bien, pero el radial y el polinomial se comportan mejor.

## Apartado d) (Bonus-3)

Ajustamos con validación cruzada sobre la variable _crim_.

```{r cvBoston, eval=F}
cv.Boston <- cv.glmnet(as.matrix(Boston[,-1]), Boston[,1], nfolds = 5, alpha = 1)
```

El error de validación cruzada es 

```{r}
cv.Boston$cvm
mean(cv.Boston$cvm)
```

Completar solución.

# Ejercicio 3

## Apartado a)

Ya tenemos cargado y separado el conjunto de datos en 80% training y 20% test.

## Apartado b)

Vamos a ajusar un modelo de Bagging. Para ello usaremos el RandomForest y le diremos que use el total de características disponibles. El error de test lo mediremos siempre con el _MSE_.

```{r bagging}
bagging <- randomForest(medv ~., data = Boston, subset = index, mtray = ncol(Boston)-1, importance = TRUE)
bagging.prediction <- predict(bagging, newdata = Boston.test)
bagging.error <- mse(medv[-index], bagging.prediction)
```

El error de test del modelo bagging es ```r I(bagging.error)```.

## Apartado c)

Ahora vamos a ajustar un RandomForest.

```{r randomForest}
randomFor <- randomForest(medv ~., data = Boston, subset = index, importance = TRUE)
randomFor.prediction <- predict(randomFor, newdata = Boston.test)
randomFor.error <- mse(medv[-index], randomFor.prediction)
```

El número de árboles usado es ```r I(randomFor$ntree)```. El error de test es ```r I(randomFor.error)```.


## Apartado d)

Ajustamos un modelo de regresión con Boosting. Como no tiene el mismo número de árboles por defecto que _randomForest_, lo ajustamos nosotros a 500, que es el parámetro por defecto del otro.

```{r boosting}
boosting <- gbm(medv ~ ., data = Boston.train, distribution = "gaussian", n.trees = 500)
boosting.prediction <- predict(boosting, Boston.test, n.trees = 500)
boosting.error <- mse(medv[-index], boosting.prediction)
```

El error de test es ```r I(boosting.error)```.

La diferencia con bagging y randomForest es notoria. Boosting se comporta mucho peor, dejando en el mejor lugar a RandomForest por muy poquito sobre Bagging. Esto es previsible, ya que _randomForest_ tiene una diversificación que no tiene _Bagging_, este tiende a sobreajustar más.

# Ejercicio 4

```{r OJ, echo = FALSE, cache = FALSE}
attach(OJ)

```

## Apartado a)

Cogemos una muestra aleatoria de 800 elementos y lo usamos como training.

```{r OJsplit}
set.seed(123456789)
index <- sample(nrow(OJ), 800)
OJ.train <- OJ[index,]
OJ.test <- OJ[-index,]
```

Ajustamos un árbol con la variable _Purchase_ como objetivo.

```{r tree}
model.tree <- tree(Purchase ~ ., data = OJ.train)
```


## Apartado b)

Veamos un resumen del árbol.

```{r summaryTree}
(model.tree.summary <- summary(model.tree))
```

El número de nodos terminales es de ```r I(model.tree.summary$size) ```, y tiene un error del ```r I(model.tree.summary$misclass[1] / model.tree.summary$misclass[2] * 100) ```%. Sin probar otros clasificadores, podemos afirmar que el árbol no es un buen clasificador para este problema, siguiendo la regla de que un clasificador se considera bueno a partir del 90% de acierto. Las variables usadas han sido ```r I(model.tree.summary$used)```.

## Apartado c)

Dibujamos el árbol obtenido.

```{r plotTree}
plot(model.tree, main="Classification tree")
text(model.tree, cex=.7)
```

La variable que más ganancia de información tiene es _LoyalCH_, los dos primeros niveles del árbol usan solamente esta variable. Además, hay algunas ramificaciones sin utilidad: Si $LoyalCH < 0.27$, no hace falta volver a distinguir para deducir _MM_, por ejemplo. 

## Apartado d)

Aplicamos el árbol a nuestros datos de test.

```{r testTree}
prediction.tree <- predict(model.tree, OJ.test, type = "class")
(error.test.tree <- confusionMatrix(prediction.tree, OJ.test$Purchase))
```

La matrix de confusión muestra todos los datos necesarios. El error de test es 1 - _Acuraccy_, ```r I(1-error.test.tree$overall["Accuracy"])``` 

## Apartado e)

Aplicamos la función cv.tree() a los datos de training y veamos qué hace.

```{r cvTree}
model.cv.tree <- cv.tree(model.tree, K = 5)
model.cv.tree
```

Para cada tamaño de árbol, calcula su error con validación cruzada. El mínimo error es el mínimo de _dev_. Lo  veremos con una gráfica en el siguiente apartado.

## Apartado f) (Bonus-4)

```{r plotcvTree}
error.tree <- data.frame(size = model.cv.tree$size,
                         error = model.cv.tree$dev)

error.tree$alpha <- ifelse(error.tree$error == min(error.tree$error), 1, 0)

ggplot(error.tree, aes(x=size,y=error)) + geom_line() +
  geom_point(aes(colour = "Mínimo", alpha = alpha )) +
  scale_x_continuous(breaks = error.tree$size) +
  guides(alpha=FALSE) +
  theme(legend.title = element_blank()) + 
  labs(title = "Cross Validation error", 
       x = "Size",
       y = "Error")
```

Podemos ver que el mínimo de error se alcanza en ```r I(error.tree$size[which(error.tree$alpha == 1)])```.