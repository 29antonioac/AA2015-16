---
title: "Trabajo 3"
author: "Antonio Álvarez Caballero"
date: "27 de mayo de 2016"
output: pdf_document
---

```{r setup, include=FALSE}
require(ISLR)
require(ggplot2)
require(plotROC)
require(plotly)
require(reshape)
require(GGally)
require(ROCR)
require(MESS)
require(boot)
require(MASS)
require(glmnet)
require(e1071)
require(caret)
require(randomForest)
require(gbm)
require(tree)
require(class)
require(knitr)

set.seed(100000004)
knitr::opts_chunk$set(echo = TRUE, cache = T)

mse <- function(sim, obs) mean( (sim - obs)^2, na.rm = TRUE)
```

# Ejercicio 1

```{r Auto, echo = FALSE, cache = FALSE}
attach(Auto)
```
```{r Auto2, echo = FALSE}
pairs(Auto)
```

## Apartado a)

Parece ser que las variables de las que más depende _mpg_ son _displacement_, _horsepower_ y _weight_. Veámoslas con más detalle.

```{r plots, echo = FALSE, cache = TRUE}
ggpairs(Auto, columns =  c("mpg","displacement", "horsepower", "weight"))
# ggplot(Auto, aes(x=cylinders,y=mpg)) + geom_boxplot(aes(group = cut_width(cylinders,0.25)))
ggplot(Auto, aes(x=displacement,y=mpg)) + geom_boxplot(aes(group = cut_width(displacement,0.25)))
ggplot(Auto, aes(x=horsepower,y=mpg)) + geom_boxplot(aes(group = cut_width(horsepower,0.25)))
ggplot(Auto, aes(x=weight,y=mpg)) + geom_boxplot(aes(group = cut_width(weight,0.25)))
# ggplot(Auto, aes(x=acceleration,y=mpg)) + geom_boxplot(aes(group = cut_width(acceleration,0.25)))
# ggplot(Auto, aes(x=year,y=mpg)) + geom_boxplot(aes(group = cut_width(year,0.25)))
# ggplot(Auto, aes(x=origin,y=mpg)) + geom_boxplot(aes(group = cut_width(origin,0.25)))
# ggplot(Auto, aes(x=name,y=mpg)) + geom_boxplot(aes(group = cut_width(name,0.25)))
```

Es claro que existe una dependencia entre estas variables. Además, con el primer plot podemos ver las correlaciones entre estas 3 variables, que es alta.

## Apartado b)

Seleccionamos las variables que hemos decidido para predecir.

```{r selection}
Auto.selected <- Auto[,c("displacement","horsepower","weight")]
```

## Apartado c)

Como nuestro conjunto de datos es grande (`r I(nrow(Auto))` instancias), podemos realizar un muestreo aleatorio. Así tampoco falseamos las muestras, cosa que podría pasarnos si realizamos un muestreo estratificado.

```{r split}
index <- sample(nrow(Auto), size = 0.8*nrow(Auto) )

Auto.train <- Auto.selected[index,]
Auto.test <- Auto.selected[-index,]
```

## Apartado d)

Vamos a crear una nueva variable, _mpg01_, la cual tendrá 1 si el valor de _mpg_ está por encima de la mediana y -1 en otro caso.

```{r mpg01}
mpg01 <- ifelse(Auto$mpg >= median(Auto$mpg), 1, 0)
Auto.selected$mpg01 <- mpg01
Auto.train$mpg01 <- mpg01[index]
Auto.test$mpg01 <- mpg01[-index]
```

### Apartado d1)

Vamos a ajustar un modelo de regresión logística para predecir _mpg01_.

```{r regLog}
model.LogReg <- glm(mpg01 ~ ., data = Auto.train, family = binomial)
prediction.LogReg <- predict(model.LogReg, newdata = Auto.test, type = "response")
prediction.LogReg.labels <- ifelse(prediction.LogReg > 0.5, 1, 0)
error.test <- sum(sign(prediction.LogReg.labels) != sign(Auto.test$mpg01)) / 
                                                  length(prediction.LogReg.labels)
```

El error de test de este modelo es `r I(error.test*100)`.

### Apartado d2)

Ahora vamos a ajusart un modelo k-NN.

```{r kNN}
scale.train <- scale(Auto.train[, colnames(Auto.train) != "mpg01"])
means <- attr(scale.train, "scaled:center")
scales <- attr(scale.train, "scaled:scale")

scale.test <- scale(Auto.test[, colnames(Auto.train) != "mpg01"], means, scales)
scale.data <- rbind(scale.train, scale.test)
scale.labels <- as.factor(c(Auto.train$mpg01, Auto.test$mpg01))

set.seed(100000004)
tune.result <- tune.knn(scale.data, scale.labels, k = 1:30, tunecontrol = tune.control(sampling = "cross"))

best.k <- tune.result$best.model$k

prediction.knn <- knn(scale.train, scale.test, Auto.train$mpg01, k = best.k, prob = TRUE)
error.test.knn <- sum(prediction.knn != Auto.test$mpg01) / length(prediction.knn)

```

El error del k-NN con $k = `r I(best.k)`$ es `r I(error.test.knn*100)`.

### Apartado d3)

Veamos las curvas ROC de ambos modelos.


```{r ROC}
roc.prediction.regLog <- prediction(prediction.LogReg, Auto.test$mpg01)
roc.performance.regLog <- performance(roc.prediction.regLog, measure = "tpr", x.measure = "fpr")

## kNN
prob <- attr(prediction.knn, "prob")
prob <- ifelse(prediction.knn == "0", 1-prob, prob)
roc.prediction.knn <- prediction(prob, Auto.test$mpg01)
roc.performance.knn <- performance(roc.prediction.knn, measure = "tpr", x.measure = "fpr")
####


roc.data <- data.frame(M = c(unlist(attr(roc.prediction.knn, "predictions")), 
                              unlist(attr(roc.prediction.regLog, "predictions"))),
                        D = c(unlist(attr(roc.prediction.knn, "labels")), 
                            unlist(attr(roc.prediction.regLog, "labels"))),
                      Model = c(rep("k-NN", nrow(Auto.test)), rep("RL", nrow(Auto.test))))


g <- ggplot(roc.data, aes(d = D, m = M, color = Model)) + geom_roc(n.cuts = 0) + style_roc()
auc <- round(calc_auc(g), 6)
g <- g + annotate("text", x = .75, y = .25, 
                  label = paste("AUC_knn = ", auc$AUC[2], "\nAUC_glm = ", auc$AUC[1]))
g


auc.regLog  <- auc$AUC[1]
auc.knn     <- auc$AUC[2]
```

El área bajo la curva de la ROC de regresión logística es `r I(auc.regLog)` y la del k-NN es `r I(auc.knn)`. Luego `r if(auc.regLog > auc.knn) "regresión logística" else "k-NN"` es el modelo que mejor _performance_ tiene.

## Apartado e) (Bonus-1)

Para estudiar el error con validación cruzada hacemos uso de `cv.glm`

```{r cv.glm}
model.full.LogReg <- glm(mpg01 ~ ., data = Auto.selected)
cv.LogReg <- cv.glm(data = Auto.selected, glmfit = model.full.LogReg, K = 5)
cv.LogReg$delta
```

El error estimado es el primero de este vector. El segundo es un ajuste para compensar el sesgo introducido al no usar *Leave-One-Out*.

Para el caso del k-NN

```{r cv.knn}
set.seed(100000004)
prediction.knn.cv <- knn.cv(scale.data, scale.labels, k = best.k, l = 0, prob = FALSE, use.all = TRUE)
error.knn.cv <- sum(prediction.knn.cv != scale.labels) / length(prediction.knn.cv)
error.knn.cv
```


Por tanto, vemos que es mejor `r if(error.knn.cv > cv.LogReg$delta[1]) "regresión logística" else "k-NN"`.

## Apartado f) (Bonus-2)

Por hacer

# Ejercicio 2

## Apartado a)

Ajustamos sobre la variable _crim_, que es la que está en la posición 1, con partición de datos de 80% y test de 20% con muestreo aleatorio.

```{r cache = FALSE}
attach(Boston)

```


```{r boston}
set.seed(100000004)
index <- sample(nrow(Boston), 0.8*nrow(Boston))
Boston.full <- Boston
Boston.train <- Boston[index,]
Boston.test <- Boston[-index,]

model.Boston <- glmnet(as.matrix(Boston.train[,-1]),Boston.train[,1], alpha = 1)

```


Ahora utilizamos un método LASSO y seleccionamos las variables que están por encima de un umbral.

```{r LASSO}
cv.Boston <- cv.glmnet(as.matrix(Boston[,-1]), Boston[,1], nfolds = 5, alpha = 1)
lambda.min <- cv.Boston$lambda.min
lasso.coeff <- predict(cv.Boston, type="coefficients", s = lambda.min)
threshold <- 0.4
selected <- which(abs(lasso.coeff) > threshold)[-1]
```

Con esto afirmamos que las características que superan nuestro umbral `r I(threshold)` son `r I(selected)`. Disminuir este umbral nos haría coger más características.

## Apartado b)

Para realizar regularización _weight-decay_ debemos usar de nuevo _glmnet_ pero con el parámetro $\alpha=0$. 

```{r weight-decay}
Boston.selected.train <- Boston.train[,selected]
Boston.selected.test <- Boston.test[,selected]

regularized.model <- glmnet(as.matrix(Boston.selected.train),
                            as.matrix(Boston.train[,1]),
                            alpha = 0, lambda = lambda.min)

regularized.prediction <- predict(regularized.model, s = lambda.min,
                                  newx = as.matrix(Boston.selected.test))
```

El error residual es la raíz cuadrada positiva del error cuadrático medio.

```{r residuals}
regularized.residual <- sqrt(mse(Boston.test[,1],regularized.prediction))
```

El error residual es `r I(regularized.residual)`.

Para ver si hay underfitting, probemos con distintos valores de lambda. 

```{r lambda2}
lambda2 <- 10*lambda.min
regularized.lambda2.model <- glmnet(as.matrix(Boston.selected.train),
                            as.matrix(Boston.train[,1]),
                            alpha = 0, lambda = lambda2)

regularized.lambda2.prediction <- predict(regularized.lambda2.model, s = lambda2,
                                  newx = as.matrix(Boston.selected.test))

regularized.lambda2.residual <- sqrt(mse(Boston.test[,1],regularized.lambda2.prediction))

```

El error residual es `r I(regularized.lambda2.residual)`.

```{r lambda3}
lambda3 <- 100*lambda.min
regularized.lambda3.model <- glmnet(as.matrix(Boston.selected.train),
                            as.matrix(Boston.train[,1]),
                            alpha = 0, lambda = lambda3)

regularized.lambda3.prediction <- predict(regularized.lambda2.model, s = lambda3,
                                  newx = as.matrix(Boston.selected.test))

regularized.lambda3.residual <- sqrt(mse(Boston.test[,1],regularized.lambda3.prediction))

```

El error residual es `r I(regularized.lambda3.residual)`.

Hemos visto que al aumentar $\lambda$ el error residual ha bajado (aunque de $10\lambda$ a $100\lambda$ no se ha movido, por lo que parece haberse estabilizado), luego podemos afirmar que hay un poco de _underfitting_ con el mínimo $\lambda$ calculado.


## Apartado c)

Al igual que en el anterior apartado, definimos una nueva variable usando la mediana como umbral.

```{r crim1, cache = TRUE}
Boston.full$crim <- ifelse(Boston$crim > median(Boston$crim), 1, -1)
Boston.train.crim1 <- Boston.full[index,]
Boston.test.crim1 <- Boston.full[-index,]
pairs(Boston.full)
```

Ahora ajustamos varias _SVM_, probaremos la lineal y con los núcleos disponibles, y veremos cómo se comporta cada uno.
Posiblemente el núcleo lineal no sea suficiente, ya que los datos no parecen lo suficientemente separados. A priori parece que uno polinomial hará un mejor trabajo.

```{r SVM}
svm.linear <- svm(crim ~ ., data = Boston.train.crim1, kernel = "linear")
svm.linear.prediction <- predict(svm.linear, newdata = Boston.test.crim1)
confusionMatrix(sign(svm.linear.prediction), Boston.test.crim1$crim)

svm.polynomial <- svm(crim ~ ., data = Boston.train.crim1, kernel = "polynomial")
svm.polynomial.prediction <- predict(svm.polynomial, newdata = Boston.test.crim1)
confusionMatrix(sign(svm.polynomial.prediction), Boston.test.crim1$crim)

svm.radial <- svm(crim ~ ., data = Boston.train.crim1, kernel = "radial")
svm.radial.prediction <- predict(svm.radial, newdata = Boston.test.crim1)
confusionMatrix(sign(svm.radial.prediction), Boston.test.crim1$crim)

svm.sigmoid <- svm(crim ~ ., data = Boston.train.crim1, kernel = "sigmoid")
svm.sigmoid.prediction <- predict(svm.sigmoid, newdata = Boston.test.crim1)
confusionMatrix(sign(svm.sigmoid.prediction), Boston.test.crim1$crim)

```

Al final, el kernel polinomial ha sido el que mejor ha funcionado. El kernel lineal se comporta muy bien, pero el radial y el polinomial se comportan mejor.

## Apartado d) (Bonus-3)

Ajustamos con validación cruzada sobre la variable _crim_.

```{r cvBoston}
cv.Boston <- cv.glmnet(as.matrix(Boston.full[,-1]), Boston.full[,1], nfolds = 5, alpha = 1, 
                       type.measure="mse")
```

El error de validación cruzada es 

```{r}
cv.Boston$cvm
mean(cv.Boston$cvm)
```

Este es el error cuadrático medio por validación cruzada del modelo.

# Ejercicio 3

## Apartado a)

Ya tenemos cargado y separado el conjunto de datos en 80% training y 20% test.

## Apartado b)

Vamos a ajusar un modelo de Bagging. Para ello usaremos el RandomForest y le diremos que use el total de características disponibles. El error de test lo mediremos siempre con el _MSE_.

```{r bagging}
bagging <- randomForest(medv ~., data = Boston, subset = index, mtray = ncol(Boston)-1, importance = TRUE)
bagging.prediction <- predict(bagging, newdata = Boston.test)
bagging.error <- mse(medv[-index], bagging.prediction)
```

El error de test del modelo bagging es `r I(bagging.error)`.

## Apartado c)

Ahora vamos a ajustar un RandomForest. Ajustaremos 2 y miraremos cuándo se empieza a estabilizar el error. A partir de ahí veremos dónde se alcanza el mínimo del error.

```{r randomForest}
randomFor <- randomForest(medv ~., data = Boston, subset = index, importance = TRUE)
ggplot(data = data.frame(Trees = 1:randomFor$ntree, MSE = randomFor$mse), aes(x = Trees, y = MSE)) + 
  geom_line()
summary(randomFor)
randomFor.prediction <- predict(randomFor, newdata = Boston.test)
randomFor.error <- mse(medv[-index], randomFor.prediction)

randomFor.optimalntree <- randomForest(medv ~., data = Boston, subset = index, importance = TRUE,
                                       ntree = which(randomFor$mse == min(randomFor$mse[100:length(randomFor$mse)])))
randomFor.optimalntree.prediction <- predict(randomFor.optimalntree, newdata = Boston.test)
randomFor.optimalntree.error <- mse(medv[-index], randomFor.optimalntree.prediction)
```

En este caso el error se estabiliza a partir de 100, más o menos. El número de árboles óptimo es `r I(randomFor.optimalntree$ntree)`. El error de test usando 500 es `r I(randomFor.error)`, y el error de test usando el óptimo es `r I(randomFor.optimalntree.error)`.

```{r echo = FALSE}
msg <- "En este caso usar el óptimo no nos ha ofrecido mejora sobre el error de test. Parece que está sobreajustando."
if (randomFor.optimalntree.error < randomFor.error)
  msg <- "En este caso usar el óptimo nos ha ofrecido mejora sobre el error de test."
```

`r I(msg)`



## Apartado d)

Ajustamos un modelo de regresión con Boosting. Como no tiene el mismo número de árboles por defecto que _randomForest_, lo ajustamos nosotros a 500, que es el parámetro por defecto del otro.

```{r boosting}
boosting <- gbm(medv ~ ., data = Boston.train, distribution = "gaussian", n.trees = 500)
boosting.prediction <- predict(boosting, Boston.test, n.trees = 500)
boosting.error <- mse(medv[-index], boosting.prediction)
```

El error de test es `r I(boosting.error)`.

La diferencia con bagging y randomForest es notoria. Boosting se comporta mucho peor, dejando en el mejor lugar a RandomForest por muy poquito sobre Bagging. Esto es previsible, ya que _randomForest_ tiene una diversificación que no tiene _Bagging_, este tiende a sobreajustar más.

# Ejercicio 4

```{r OJ, echo = FALSE, cache = FALSE}
attach(OJ)

```

## Apartado a)

Cogemos una muestra aleatoria de 800 elementos y lo usamos como training.

```{r OJsplit}
set.seed(100000004)
index <- sample(nrow(OJ), 800)
OJ.train <- OJ[index,]
OJ.test <- OJ[-index,]
```

Ajustamos un árbol con la variable _Purchase_ como objetivo.

```{r tree}
model.tree <- tree(Purchase ~ ., data = OJ.train)
```


## Apartado b)

Veamos un resumen del árbol.

```{r summaryTree}
(model.tree.summary <- summary(model.tree))
```

El número de nodos terminales es de `r I(model.tree.summary$size)`, y tiene un error del `r I(model.tree.summary$misclass[1] / model.tree.summary$misclass[2] * 100)`%. Sin probar otros clasificadores, podemos afirmar que el árbol no es un buen clasificador para este problema, siguiendo la regla de que un clasificador se considera bueno a partir del 90% de acierto. Las variables usadas han sido `r I(model.tree.summary$used)`.

## Apartado c)

Dibujamos el árbol obtenido.

```{r plotTree}
plot(model.tree, main="Classification tree")
text(model.tree, cex=.7)
```

La variable que más ganancia de información tiene es _LoyalCH_, los dos primeros niveles del árbol usan solamente esta variable. Además, hay algunas ramificaciones sin utilidad: Si $LoyalCH < 0.05$, no hace falta volver a distinguir para deducir _MM_, por ejemplo. 

## Apartado d)

Aplicamos el árbol a nuestros datos de test.

```{r testTree}
prediction.tree <- predict(model.tree, OJ.test, type = "class")
(error.test.tree <- confusionMatrix(prediction.tree, OJ.test$Purchase))
```

La matrix de confusión muestra todos los datos necesarios. El error de test es 1 - _Acuraccy_, `r I(1-error.test.tree$overall["Accuracy"])` 

## Apartado e)

Aplicamos la función cv.tree() a los datos de training y veamos qué hace.

```{r cvTree}
model.cv.tree <- cv.tree(model.tree, K = 5)
model.cv.tree
```

Para cada tamaño de árbol, calcula su error con validación cruzada. El mínimo error es el mínimo de _dev_. Lo  veremos con una gráfica en el siguiente apartado.

## Apartado f) (Bonus-4)

```{r plotcvTree}
error.tree <- data.frame(size = model.cv.tree$size,
                         error = model.cv.tree$dev)

error.tree$alpha <- ifelse(error.tree$error == min(error.tree$error), 1, 0)

ggplot(error.tree, aes(x=size,y=error)) + geom_line() +
  geom_point(aes(colour = "Mínimo", alpha = alpha )) +
  scale_x_continuous(breaks = error.tree$size) +
  guides(alpha=FALSE) +
  theme(legend.title = element_blank()) + 
  labs(title = "Cross Validation error", 
       x = "Size",
       y = "Error")
```

Podemos ver que el mínimo de error se alcanza en `r I(error.tree$size[which(error.tree$alpha == 1)])`.